1.1

Classes of Computers
    Desktop
        designed for use by an individual
        incorporates a graphics display, keyboard, and mouse
    Server
        used for running larger programs for multiple users
        typically accessesd only via a network
        widest range in cost and capability
    Supercomputer
        class with the highest performance and cost
        configured as servers
        oriented towards carrying a large workload
    Datacenters
        similar to supercomputers
        considered as large clusters of computers
    Embedded Computers
        the largest class of computers
        widest range of applications and performance

Components of Program Performance
    Algorithm
    Programming Language
    Programming Compiler
    Programming Archetecture
    Processor and Memory System
    I/O system (hardware and operating system)

Kilobytes and Kibibytes
    kilobyte | KB | 10^3 || kibibyte | KiB | 2^10
    megabyte | MB | 10^3 || mebibyte | MiB | 2^10
    ...


1.2

Software Layer Hierarchy
    from high to low level:
    Applications Software
    Systems Software
        Operating System
            interfaces between a user's program and the hardware
            provides supervisory functions and services, including:
                handling I/O operations
                allocating storage and memory
                providing protected sharing of the computer amongs applications
        Compiler
            translates a program into instructions that the hardware can
                execute
    Hardware

Translation to Hardware Instructions
    Binary
        "Machine Language"
        this is the lowest level computer language
        originally computers were instructed in binary by hand
    Assembler
        "Assembly Language"
        this was the first computer compiler
        a simple program that translates a symbolic instruction to binary
        requires the programmer to write one line of code for each hardware
            instruction
    Compiler
        high level languages still used today
        translate powerful language into assembly

Scope of Computer Architecture
    Level 0: Materials Technology
        what materials should the basic devices be made of?
        not in the scope of this class
    Level 1: Device Technology
        design/fabrication of basic logic/memory elements
            transistors, logic gates, flip-flops
            very large scale integration (VLSI)
    Level 2: Logic Design
        design of logical circuits from basic gates and memory devices
        combinational logic
            adders, multipliers, decoders, muxes, demuxes, etc
        sequential logic
            FSA's, counters, sequence detectors, etc
        memory design
            construction of SRAM, DRAM, Register Files, etc from smaller
                elements
    Level 3: Processor-Memory Organizatoin
        ALU design using basic combinational logic
        CPU design
            control uit design (FSA design) using sequential logic
            registers
        memory system organization
            cache, DRAM, ROM, Virtual Memory
    Level 4: Instruction Set Architecture (ISA) Design
        design of assembly language
            operations/operands permitted
            addressing modes for operands
    Level 5: Generic Parallelization and Optimization
        instruction level parallelism
            pipelining of instructions
        memory latency reduction
            multi-level caching
            virtual memory
        multi-threading
            execude multiple threads concurrently each on an independent core
        data parallelism
            execute a single instruction or process concurrently on multiple
                chunks of data
    Level 6: Application-Specific Parallelization and Optimization
        numerical computing
            matrix operations, finite element computation
        graphical computing
        data-intensive computing
            focus on info stroage/retrieval, I/O, db systems, etc
        AI/Cognitive Computing
            speech recognition, pattern recognition
            


1.3

Five Components of a Computer
    Input
        keyboard, mouse
    Output
        screen
    Memory
    Datapath
    Control
        datapath and control are parts of the processor

Displays
    each image is composted of picture elements (pixels)
        these can be represented in a matrix called a bitmap
    each pixel has three color values for red, blue, and green
        8-bit displays use 8 bits for each color
            24 bits in each pixel
            2^24 colors possible
    hardware for graphics includes a raster refresh buffer (frame buffer)
        stores the bit maps for the images

DRAM and SRAM
    volatile forms of memory
    information stored is lost on shutdown
    this is because the memory is stored electronically, not magnetically
    about 100,000x faster than secondary storage
    Dynamic RAM
        contains the instructions and data of a program
        "random" means that all memory access takes the same amount of time
    Static RAM
        cache memory
        small, fast memory that acts as a buffer for DRAM memory
        less dense and more expensive than DRAM

Processors
    follows the instructions of the program
    contains two main components:
        Datapath
            performs arithmetic operations
        Control
            tells the datapath, memory, and I/O devices what to do according to
                the program

Performance
    performance is difficult to measure because:
        there are many variables to account for
        there are many metrics by which to evaluate performance
            different metrics may be relevant to different applications of the
                machine being evaluated
    ultimately, we are mostly interested in execution time (and sometimes
        throughput)

Measuring Performance
    Components of Performance | Units of Measure
    --------------------------------------------------------------------------
    CPU execution time        | Seconds for the program
    Instruction count         | Instructions executed for the program
    CPI                       | Average number of clock cycles per instruction
    Clock cycle time          | Seconds per clock cycle

Components of Program Performance
    Algorithm
        affects IC, possibly CPI
        determines the number of program instructions executed
        may affect CPI by favoring slower or faster instructoins
        can change IC based on flow of control in the algorithm
    Programming Language
        affects IC and CPI
        translates statements to processor instructions that determine IC
        features may affect performance (e.g. java vm)
    Processor and Memory System
        affects IC and CPI
        determines the translation into computer instructions
    ISA
        affects IC, CR, CPI
        affects the instructions needed for a function
        determines cost in cycles for each instruction
        determines the clock rate for the processor


Comparing Performance
    P  = performance
    ET = execution time
    for two computers:
        P(x) = 1/ET(x)
    so if:
        P(x)    > P(y)
    then:
        1/ET(x) > 1/ET(y)
        ET(y)   > ET(x)

    to say "x is n times as fast as y",
        P(x)/P(y)   = n
        ET(y)/ET(x) = n

Execution Time
    Elapsed Time
        time the user by the user
    CPU execution time (CPU time)
        time the CPU spends computing for the task
        does not include time waiting for I/O or running other programs
        User CPU Time:
            CPU time spent in the program
        System CPU Time:
            cpu time spent in the OS performing tasks on behalf of the program
    system performance refers to elapsed time on an unloaded system
    CPU performance refers to user CPU time

CPU Clock
    ticks in the cpu clock are called clock cycles
    a clock period is the time for a complete clock cycle
    the clock rate is the inverse of the clock period
        measured in GHz

    MORE ON CLOCKING

CPU Performance
    ET = CPU execution time for a program
    CC = CPU clock cycles for a program
    CR = clock rate

    ET = CC / CR

Instruction Performance
    execution time also depends on the number of instructions in the program

    IC  = instruction Count
    CPI = average clock cycles per instruction

    CC = IC * CPI
    
    ET = IC * CPI * CCT
    ET = (IC * CPI) / CR


1.6

The Power Wall


1.8

Amdahl's Law

MIPS


2.2

Operations of Computer Hardware
    most basic operation is arithmetic
    addition
        add
        ex: add a, b, c
        a = b + c
        must always have exactly three variables
    subtraction
        sub
        the same as addition
    add immediate
        addi
        "add immediate"
        used to add constants
    load word/half/half unsigned/byte/byte unsigned/linked word/ upper immed.
        lw/lh/lhu/lb/lbu/ll/lui
        loads the given object from memory to register
    store word/half/byte/condition. word
        sw/sh/sb/sc
        store the given object from register to memory
    and/or/not
        and/or/nor
        bitwise operations
    and/or immediate
        andi/ori
        bitwise operations for constants
    shift left/right logical
        sll/srl
        shift bits left or right by constant
    branch on equal/not equal
        beq/bne
        equal/not equal test
    set on less than (unsigned/immediate/immediate unsigned)
        slt/sltu/slti/sltiu
        compare less than
    jump (register/ and link)
        j/jr/jal
        jump to target address

Examples
    !!!


2.3

Registers
    limited number of small storage spaces for quick access
    implemented in hardware
    limited in number
        32 registers on MIPS
    each register is 32 bits
        groups of 32 bits are called a word in MIPS
    the three operands of MIPS arithmetic instructions must be chosen from one
        of the 32 32 bit registers

Design Principle 2: Smaller is faster
    a large number of registers may increase the CCT
        signals have to travel farther
    "smaller is faster" is not absolute
        31 registers may not be faster than 32
    this principle is still useful enough to be taken seriously

Compiling C Using Registers
    ex:
        f = (g + h) - (i + j);
        // f,g,h,i,j = $s0, $s1, $s2, $s3, $s4

        add $t0, $s1, $s2
        add $t1, $s3, $s4
        sub $s0, $t0, $t1

Memory Operands
    larger structures can not be stored in registers
        arrays, data structures
        these are stored in memory instead
    MIPS includes data transfer instructions to access memory

Data Transfer Instructions
    to access a word in memory, the instruction must supply the memory address
        memory is a large, 1D array, where the address is the index
    MIPS provides instructions for handling this
    Load:
        copies data from memory to a register
        "lw" instruction
        ex: 
            # base addres of A is stored in $s3
            lw $t0, 32($s3)  # $t0 gets A[8]
    Store:
        copies data from a register to memory
        "sw" instruction
        ex:
            sw $t0, 48($s3) # stores $t0 into A[12]
        

Byte Addressing
    most addressing is done using bytes rather than bits
    addresses of different elements of an array will differ depending on the
        type of data structure stored in the array
        e.g. ints will be stored in groups of 4 bytes
        in MIPS, addresses for words must be multiples of 4
            this is called an alignment restriction



Endianness
    Big Endian
        use the leftmost or "big end" byte as the word address
        MIPS is big-endian
    Little Endian
        use the rightmost or "little end" byte as the word address


2.4


2.5

Registers
    Variable | Number(s) | Use              | Preserved on Call
    -----------------------------------------------------------
    $zero    | 0         | Constant value 0 | n.a.
    $v0-#v1  | 2-3       | Return values    | no
    $a0-$a3  | 4-7       | Arguments        | no
    $t0-$t7  | 8-15      | Temporaries      | no
    $s0-$s7  | 16-23     | Saved            | yes
    $t8-$t9  | 24-25     | More Temporaries | no
    $gp      | 28        | Global Pointer   | yes
    $sp      | 29        | Stack Pointer    | yes
    $fp      | 30        | Frame Pointer    | yes
    $ra      | 31        | Return Address   | yes

Fields
    6,5,5,5,5,6 bits
    op:    opcode
    rs:    first register source operand
    rt:    second register source operand
    rd:    register destination operand
    shamt: shift amount
    funct: function
        selects the specific variant of the operation in the op field

Register Formats
          6     5     5     5     5     6
    R:  | op  | rs  | rt  | rd  |shamt|funct|
    I:  | op  | rs  | rt  |   addr/const    |


2.6

Shifts
    move all the bits in a word left or right
        fill emptied bits with 0's
    ex:
        sll $t2, $s0, 4  # reg $t2 = reg $s0 << 4

        op  rs  rt   rd   sh  fu
        0 | 0 | 16 | 10 | 4 | 0


2.7

Flow Control
    ex: 
        if (i==j) {} else {}

        bne $s3, $s4, Else

Loops
    ex:
        while (save[i] == k)
            i += 1;

        Loop: sll   $t1, $s3, 2    # $t1 = i * 4
              add   $t1, $t1, $s6  # t1 = address of save[i]
              lw    $t0, 0($t1)    # t0 = save[i]
              bne   $t0, $s5, Exit # Exit if save[i] != k
              addi  $s3, $s3, 1    # i = i + 1
              j     Loop           # go to Loop
        Exit:


2.8

Procedure Steps
    1. Store parameters
    2. Transfer control to procedure
    3. Acquire storage resources for procedure
    4. Perform task
    5. Store result
    6. Return control to the point of origin

Procedure registers
    $a0-$a3: four argument registers for parameters
    $v0-$v1: two value registers for return values
    $ra:     points to the address of the point of origin
        jal command jumps to an address and links the point of origin to $ra
            the PoA is actually PC + 4, so it returns the address after the
                procedure call
        jr $ra returns to the point of origin


Leaf Procedures
    ex:
        int leaf_example (int g, int h, int i, int j) {
            int f;
            f = (g + h) - (i + j);
            return f;
        }

    leaf_example:
            addi $sp, $sp, -12 # adjust stack to make room for items

            sw $t1, 8($sp)      # save $t1 for use afterwards
            sw $t0, 4($sp)      # save $t0 for use afterwards
            sw $s0, 0($sp)      # save $s0 for use afterwards

            add $t0, $a0, $a1   # register $t0 contains g + h
            add $t1, $a2, $a3   # register $t1 contains i + j
            sub $s0, $t0, $t1   # f = $t0 - $t1
            add $v0, $s0, $zero # returns f ($v0 = $s0 + 0)

            lw $s0, 0($sp)      # restore $s0 for caller
            lw $t0, 4($sp)      # restore $t0 for caller
            lw $t1, 8($sp)      # restore $t1 for caller

            addi $sp, $sp, 12   # delete items form stack

            jr $ra              # jump back to calling routine

Nested Procedures
    ex:
        int fact (int n){
            if (n < 1) return(1);
                else return n* fact(n - 1);
        }

    fact:
        addi $sp, $sp, -8   # adjust stack for 2 items
        sw   $ra, 4($sp)    # save the return address
        sw   $a0, 0($sp)    # save the argument n

        slti $t0, $a0, 1    # test for n < 1
        beq  $t0, $zero, L1 # if n >= 1, go to L1

        addi $v0, $zero, 1  # return 1
        addi $sp, $sp, 8    # pop 2 items off stack
        jr   $ra            # return to caller

    L1: addi $a0 $a0 -1     # n >= 1: argument gets n - 1
        jal  fact           # call fact with n - 1
        
        lw   $a0, 0($sp)    # return from jal: restore argument n
        lw   $ra, 4($sp)    # restore return address
        addi $sp, $sp, 8    # adjust sp to pop 2 items

        mul  $v0, $a0, $v0  # return n * fact(n - 1)

        jr   $ra            # return to caller
            

The Stack


Preserved and Non-Preserved Registers

Global Pointer

Frame Pointer

The Heap

Recursion


2.9


2.10


