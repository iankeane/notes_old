Ian Keane

---------------
Virtual Reality
---------------


Tue Jan 12 09:24:10 EST 2016
----------------------------

Required tools:
    google cardboard
    joystick
    unity3d
    oculus rift dk1 or dk2?

research: native plugins


Thu Jan 21 12:32:04 EST 2016
----------------------------

Unity Basics
    unity projects are just folders
    four folders in total
        Assets
            jpgs and other objects for the game
        Library
            converts assets for platform
        ProjectSettings
            information about how the project is organized
        Temp
            can be deleted
    projects are arranged into scenes
        scenes are files
        first action should be to save the default scene as something
        scenes can be used as levels, or as layers of a level
            ex: can have an enemies scene, empty level scene, etc
    views
        perspective view
            useful for seeing things the way the eye will see
        orthographic view
            useful for lining things up
    layers
        can be adjusted to have different properties
    local and global space
        programmatically, we are always working with local space
        local/global space for transformation can be toggled in the editor
    inspector window
        can be locked to one object (useful for moving other things in the
            scene while editing the first)
        contains all the components of the object
            everything in the game is an object
            an object is just a container for components
        objects can be tagged in the inspector window
            sometimes tagging can lead to sloppy code
        layers are edited in the inspector window as well
    components
        transform
            cannot be deleted, every object has a transform
            contains position, rotation, and scale
            best to edit to round values
                hold command to snap to grid and snap rotation
        mesh filter
            creates the mesh itself
        mesh renderer
            controls the material for the mesh
            every mesh renderer has a shader
                albedo modifies color
                unity only allows editing in 8-bit color
        camera
            controls whether the skybox is drawn or not
            clipping plane is set here (draw distance)
                affects the depth buffer
                also controls how close you can see something
            viewport rectangle can be used to draw multiple cameras or limit one
                camera to part of the screen
                can be used, for example, to make a rearview camera
            target texture
                this is how you make render textures
    lighting
        no lighting by default
        light seen in the scene is "ambient lighting"
            edited under window > lighting
            affects how well lit objects look
            skybox has ambient lighting as well
    unity game loop
        see diagram
        update
        coroutines
        animation
        lateupdate
        rendering
        coroutines
            yield WaitForEndOfFrame
        also see basic unity monobehavior diagram
            disregard ongui
    console window
        prints debugging information

Scripts
    a script is a component on an object
        it is a good idea to have a script that has the same name as the object
    it is a good idea NOT to try to change the name of a script after creation

UnityScript Functions
    Start()
    Update()
    time.deltatime
    debug.log
        prints to the console


    **Note: in lighting window, turn off "auto" under other settings


Tue Jan 26 12:26:19 EST 2016
----------------------------

Scene Organization in Unity
    heirerarchy management
        pivots can be altered by moving an object under a parent object
        order in a heirarchy affects the draw order
            this can affect transparency
    snowman example:
        Snowman
            Base
            Graphics
                Torso
                Graphics
                    head
                    Graphics
    this allows for the most scaling control
    animation
        it may be batter to animate outside of unity
        mechanim is the default unity system
            create>animatoionController
                uses a state machine
            add animator to snowman
            attatch animationController to animator
        different parts are animated separately
            grouping by object can help control animations
        create animations
            jump/idle
        set idle to default state
        make a transition to trigger jump, transition back to idle
            add trigger in parameters window
            add as a condition in inspector window
        note: make transition duration 0 or .01
            for idle: set to loop
            for jump: exit time = 1
        open animation window
            remove the default thing
        change rotation, scale, and transformation over time to create animation
        colliders and raycasts move with animations

GUIs
    create canvas
        add things like button
        objects can be anchored to edges of the canvas
    create GUIHandler script
        attatch to the objects on the canvas
        public void handlejumpbuttonpress(){}
        snowmanbehavior public void dojump()
            {this.getcomponent.etc }
        in inspector button>add component>onmouseclick>handlejumpbuttonpress

Rendering
    The proceess of drawing objects based on their geometry and surface
        charactaristics
        objects are usually drawn to a 2D screen window
        considered as a rectangular array of pixels
    This differes from geometry, which is about the vertices and faces
    Two steps:
        Vertex Operations to find space polygons
        Surface Operations to determine how those polygons appear on the screen
            this is done using triangles
            triangles in 3D space are the same as in 2D space
        both steps are caculated in parallel
    there is no concept of camera space in a graphics engine
        all that is stored is the camera location
        viewpoints are controlled by moving around cameras
        camera view is calculated separately from everything else
    scene is drawn to a portion of the screen called the viewport
        this drawing is then clipped if wen they lie outside the bounds of the
            camera
    another step in the rendering pipeline is the vertex color
        even though a vertex has no geometry it can have a color
        each vertex has to consider its color in respect to the lighting
    viewports are separated into two parts:
        Near Plane:
            everything too close will be clipped out
        Far Plane:
            everything too far will be clipped out
        things loaded between these two are loaded in the "depth buffer" (Z
            buffer)
        each depth buffer has a resolution
            typically 32, sometimes 24 or 16
            32 is four billion positions that something could be in the buffer
            depth fighting can happen when it is unclear what is in front or
                behind another object in space
                sometimes you get depth fighting with posters on walls, etc
            using a depth buffer can help this
            typically, if you need something 3D that's very far away, another
                camera is used that is drawn first in the scene
            you can set your own clipping planes in the camera inspector
    Projection can be done in two ways:
        Perspective
            points projected towards the viewpoint, for realism
            things look smaller the farther they are from the camera
        Parallel "orthographic"
            points are projected perpendicularly to the screen
            their size is not affected by distance
            good for 2D games
        this can also be set in the camera settings
    shaders affect both vertex and surface operations

Surface Operations
    gpus consider each triangle separately, and builds them one at a time
    backward rendering is how movies render things
        starts with an object and uses rays to feel out projection
        this is extremely slow

Lighting
    Any light in a scene has a position, volume,direction, intensity, and color
        surface colors (color texture)
        surface directions (normal texture)
        generally cheaper to render than more triangles


Thu Jan 28 12:31:53 EST 2016
----------------------------

Local Lighting
    Everything you can do with lighting and object without knowing about 
        the polygons around the object
        this is cheaper than dynamic lighting
        often involves a shadow map or environment map
    local lighting includes:
        surface colors
        surface reflectivity (specular texture)
        surface directions (normal texture)
    global lighting includes:
        reflection
        shadows
        indirect lighting
        refraction/translucency
    all unity rendering is forward rendering
    backward rendering is sometimes called ray tracing

Lighting Options in Unity
    edit>lighting settings>quality
        important settings for optimizing a game
        can set vertex/pixel lighting
            pixel count=0 is vertex lighting
        disabling antialiasing can remove a lot of overhead
            usually antialiasing is important for VR
        shadows are extremely expensive
        vsync is set here
    light objects
        can be individually set to cast or not cast shadows
        a culling map can prevent certain objects from being lit
    directional light
        the "sun"
        all light rays come from one direction
        cheapest light to render
        position is irrelevant
            only rotation affects the directional light
        normally there is not more than one directional light in an outdoor
            scene
        multiple directional lights look like a sports arena
            creates a circle of shadows
    point light
        opposite of directional light
            orientation does not matter, only position
    spotlight
        used to create light in a radius
    baked light
        setting an object to static and baked removes a lot of work
        affects the global lighting settings
            the floor should also be set this way
        !!!
    specular highlights
        view dependent
        !!!

shadows
    shadow distance:
        lower shadow distance makes the sharpest shadows
    bias:
        determines where a shadow starts drawing
        used for thin objects
        !!!
    hard shadows:
        !!!
    soft shadows:
        !!!

shading
    shading is different from shadowing
        just darkens or lightens based on which side the light is facing
    unity has a default unifying shader
    rendering modes:
        opaque:
            !!!
        cutout:
            !!!
        fade:
            !!!
        transparent:
            avoid shadowing and transparency simultaneously
            !!!
    main maps
        albedo:
            main color of object
                secondary albedos can be added somehow !!!
        metallic:
            affects the specularity
        normal maps:
            !!!
        height maps:
            !!!
        occlusion maps:
        emission:
            causes object to emit light
            can be baked
            can affect other objects or not
        detail maps:
    shading can cause the faces of the object to become more obvious
        this is bad for round objects
        this can be sort of improved using normal mapping (too intensive)
    changing shaders to vertex lit can improve performance


Primitives
    quad
        made from two triangles
        flat surface, rectangular
        back is invisible
    plane
        made from many triangles
        flat surface, rectangular
        back is invisible

Visible Surface Determiniation
    problem that surfaces are drawn in some order, usually in parallel
        VSD is the problem of ensuring the right surfaces are on top
    solutions:
        painter's algorithm:
            drawing from back to front
            very expensive algorithm
            nearly perfect, solves many issues
        z-buffering (depth-buffering):
            pixel is drawn only if it is closer to the viewpoint than the current
                pixel
            produces problems with translucency
                this is because you can only check one value at a time
                it would require a separate buffer for every translucent object
        backface culling:
            only draws surfaces that "face" the camera
            does this by determining which direction each triangle is faceing

Double Buffering and VSync
    double buffering separates the act of drawing a screen to a physical display
        from rendering the screen pyxels
    VSync makes drawing only happen every time your monitor is refreshed
        VSync turning off can cause visual tearing


Tue Feb  2 12:24:21 EST 2016
----------------------------

Cannon Game
    variables
        make public transform turntable
            assign in inspector
        rotation speed = 90.0f
    input
        bool ccwinput = Input.GetKey(KeyCode.A)
        bool cwInput, pitchUpIntput, pitchDownInput
    turn behavior
        float turntablevel = 0.0f
        if(ccwinput)
            turntablevel += rotationspeed
        if(cwinput)
            turntablevel -= rotationspeed
        float turnincrement = turntablevel * Time.deltaTime
        turntable.Rotate(new Vector3(0,turnincrement,0), Space.self)
    camera
        attatched to the player
            player is attatched to cannon
        change clipping behavior if needed
    shooting
        create nozzle object on cylinder
            rotate -90 degrees (z axis is straight out)
            shoot toward z axis
                this makes vector3.right,forward,left work intuitively
        create fire function
            public GameObject projectilefab
                projectilefab will be a prefab attatched from editor
            fire instantiates projectile
                GameObject projectile =
                GameObject.Instantiate<GameObject>(projectilefab)
                projectile.transform.position = nozzle.position;
                projectile.transform.rotation = nozzle.rotation;
                projectile.transform.localScale = new Vector3.one;
                    it's good practice to set scale at instantiation
        put rigidbody where cannon script can access it
            projectile.GetComponent<Rigidbody> ().velocity = speed *
                nozzle.foreward
            to control speed, take as fire parameter from input
        calling fire
            called in player input system
                create reference to cannon script attached to cannon and drag in inspector
                public cannon cannon
            if(input.getkeydown(keycode.space))
                cannon.fire()


Physics
    physics are added by adding a rigidbody component
        rigidbodies have mass, which determines momentum
        conservation of momentum means that the more the mass, the more the
            reaction on collision


Thu Feb  4 12:25:32 EST 2016
----------------------------

Collision
    During collision:
        forces are applied to rigidbodies
        scripts attached to objects with rigidbodies are checked
    OnCollisionEnter gives us acces to:
        the collider that was hit
        the rigidbody that was hit
    Kinematic collision property
        this exempts the object from physics except for:
            affecting other objects
            scripts updating the physics

State
    includes:
        position/orientation of the mesh
        position of every vertex in the mesh
        scene heirarchy
        velocity/acceleration of objects
        variables
    state includes everything that needs to be updated
    each state variable has an update method that changes part of the current
        state given current inputs and time to simulate for


Tue Feb  9 12:28:19 EST 2016
----------------------------

What an Enigne Does
    most important thing a game engine does is run the game loop
    within the game loop, the engine handles everything else
    input management
        local devices and networking
    asset management
        load meshes, textures, sounds, etc
    state management
        collision/physics
    rendering management
        turns data into images/sound

Collision Detection and Physics
    object data is represented in a file
        size, shape, mass, etc
    this data must interact with data outside of the object
        e.g. gravity, wind, etc
    physics checks per second can be edited in time settings in unity

Dynamics
    determine how the objects change with subject to forces
        use separating axis theorem to check if touching
        if a plane can fit between them, they are not touching
    this is simple for "rigid bodies"
        acceleration = force/mass
        angular acceleration = torque/inertia
        joints and springs can be added to rigidbodies
            joints constrain motion, springs add motion
    it is more difficult for "soft bodies" like cloth, ropes, hair
        usually simulated with springs between vertices
        self-collision is extremely complex and rarely implemented
        unity3D calls these "cloth" colliders

Collision Detection
    works by moving everything and checking intersection
        collision dection is really "intersection detection"
    the graphics in unity are completely separate from the physics
    the simplest collider is a sphere collider
        this should be used whenever collider shape is arbitrary
    colliders in order of computational complexity:
        sphere, box, capsule, compound, mesh
    compound meshes
        made by adding colliders to sub-objects of a parent
        this is usually preferred
    most collision detecton is done frame by frame or similar
        this is called discrete collision detection
        this can cause extremely fast moving objects to skip through others
    continuous collision detection is the alternative
        if an object has not been checked, checks every time an object moves
            its width
        this can be enabled in the rigidbody menu
        do not use unless it is necessary
            may be necessary for small or fast objects

Collision Response
    this is what happens after collision
    basic algorithm:
        back up object to the point in time of the collision
        compute new forces for each object based on geometry and materials
        simulate motion for the remaining time
    unity mostly just uses the velocities of the object


idea for project:
    take disaster tweets
    propogate around player in real time
    allow player to grab
    high apartment with skybox



Thu Feb 11 12:32:22 EST 2016
----------------------------

Google Cardboard
    mostly use the cardboardMain prefab
    in external tools, need the most recent jdk
    adjust accelerometer hz to improve tracking

Gamepad
    edit player input system script (cannon)
        comment out mouse movement
        float turntableRotationInput = Input.GetAxis("Horizontal");
        float turntableIncrement = rotationSpeed * turntableRotationInput * Time.deltaTime;

Data Access in Unity
    C# file IO
    Create StreamingAssets folder
        put csv in folder
        creat DataManager script
        using System.IO;
        ReadAllLines;

Raycasting
    part of the physics engine that allows simple intersection testing
    a ray consists of a starting base position and a direction (unit vector)
        unity provides methods for detecting:
            first thing the ray hits
            a list of things the ray hits


Tue Feb 16 12:26:17 EST 2016
----------------------------

Prefabs
    to edit in scripts
        make a public GameObject variable for the prefab
        assign prefab to that variable in editor
        call GameObject.Instantiate<GameObject>(prefab);
        set prefab.transform.position
        set prefab.transform.rotation

VR Tracking
    typically pose is tracked
        pose is the position and orientation of an object
    tracking provides feedback from the real world
    problems include:
        latency
        refresh rate
        calibration
        resolution
        depth of field
        encumberance

Sensor Model
    gain and bias are common in measurement
        gain is scale, and bias is shift
        they both affect each other, and it is difficult to distinguish which
            is affecting position
    noise is a margin of error in tracking
        noise is created by both gain and bias

Process Model
    simulation updates
    includes calculations and assumptions to aid representation
        this includes predicting motion to smooth animation

Filtering
    filtering is incorporation of information from both models
        window filter
            stabilize object by averaging the last N readings
        exponential filter
            perform a weighted average between the current estimation and the
            new value
    filtering can cause latency
        the calculations are not believed immediately
        interperetation of data causes things to slow down
        the more accurate the readings, the less filtering and latency
    
Basic VR Trackers
    Mechanical (oldest)
        based on sensing mechanical changes
        gloves, arm linkages
        effectively no latency
    Optical
        Use light sources to sense properties from a distance
        data intensive, low encumberance
    Acoustic
        similar to optical trackers, but use microphones and speakers
        less interference than optical, but requires more power
    Magnetic
        use pulsed magnetic field sources and sensors to determine relationship
            between source and sensor
        magnetic fields pass through non-metallic substances
    Inertial


Thu Feb 25 12:31:24 EST 2016
----------------------------

VR Peripheral Network
    designed for common tracking systems over IP networks
    uses the client-server model
        trackers serve data to clients
    tracking devices are a set of
        buttons
        position/orientation trackers
        analog values
    to work with VRPN devices, you need
        a server with VRPN interface
        IP address and port of the server
        name of the VRPN device
        type of the VRPN device
    the name, IP, and port are included together
        eg name@192.168.2.1:3883
            3883 is the standard VRPN port
        the name is case sensitive


Thu Mar  3 12:30:02 EST 2016
----------------------------

Vuphoria Platform
    does not work with latest unity
        5.3.1f1 works

Immersive Audio
    sound is a pressure wave moving through a system
        can't hear sound in space
        human range is 20Hz to 20KHz
    humans can sense spatial properties of sounds
        this is because we have two ears
            this cannot be accomplished using two microphones
    unity can generate a stereo sound signal
        this requires a spatial transform
        simple stereo sound controls volume based on which side the object is
            on (left, right)
        the relative velocity of a sound also changes its frequency (doppler
            effect)

3D Aural Displays for VR
    in order of preference
        high quality in-ear microphones in a quiet environment offer the best
            experience with an HRTF rendering process
        high quality surround sound studio headphones that cover the ear and
            block ambient sound
        surround sound speakers
        over-the-ear stereo headphones
        on-ear headphones
        simple stereo speakers


Tue Mar 15 12:24:15 EDT 2016
----------------------------

look into: mirrorop desktop remote receiver

Immersive Rendering
    head pose determines eye position
        can't track eyes because of complexity
    eye position determines perspective point
    
Volumetric Displays
    sometimes called 2.5D
        2D images that seem 3D
        sometimes 3D projection can be emulated with a rotating mirror
        2.5D is the best we can do currently with headmounted displays

Depth Effects
    we can include 'cues'that give our brain 3D information
        creates perspective that relies on our experience viewing 3D objects
         +---+
        /   /|
       +---+ +
       |   |/
       +---+
    there are about ten cues that give us depth:
        Monoscopic Depth Cues (single 2D image) [6]
            Interposition                     +---+
                an occluding object is closer |   |
            Shading                           | +---+
                shape and shadows             +-|   |             +---+
            Size                                +---+            +---+|
                the larger object is closer                 +-+  |   |/
            Linear Perspective                             +-+ | +---+ 
                parallel lines converge at a single point  +-+/    
                higher the object is vertically, the further it is ````----____
            Surface Texture Gradient                                -----------
                more detail for closer objects                      ___----````
            Atmospheric Effects
                further away objects are blurrier and dimmer   ````****####%%%%
        Stereoscopic Depth Cues (two 2D images) [1]            ````****####%%%%
            Steropsis                                          ````****####%%%%
                same as google cardboard technique
                not necessarily as important as doing other cues well
                components required for stereopsis:
                    eyepoint
                    look-at point
                    field-of-view
                    view up direction
        Motion Depth Cues (series of 2D images) [1]            
            Parrallax
                able to use motion to make depth apear to change
        Physiological Depth Cues [2]
            Accommodation
                focusing adjustment to change the shape of the lens
            Convergence
                movement of the eyes to bring something into focus !!!
        often monoscopic cues are more important than people assume

Problems with Stereoscopic Images
    Convergence
        !!!
    Camera Frustrum
        most people have a ~60mm pupillary distance
        outliers have problems with stereoscopic VR that is not calibrated to
            them
    Position Dependence
        this necessitates head tracking
        fixed display VR requires more advanced methods
    Interocular Dependence
        depth can become incorrect if eyes are modeled closer than they are in
            real life
            causes discrepancy between perceived point and modeled point
            too close = compressed world
            too far   = elongated world
    
Time Multiplexing
    time parallel steroscopic displays display two images separately
        each eye sees a different screen
    time multiplexed stereoscopic displays display alternating images with
        slightly different perspectives
        requires powered glasses with alternating shutters

Polarization
    linear polarization
        tilts light in different directions
            glasses can mask light coming from certain directions
        ghosting increases when you tilt head
        reduces image brightness by about 1/2
        potential problems with multiple screens
        not used much anymore
    circular polarization
        more confusing, uses spinning light in a direction
            each lense can only see clockwise or counterclockwise spinning
                light
        reduces ghosting
        reduces brightness
        reduces crispness
        most common form of polarization today
        always used for multiple display walls
    time multiplexing usually produces a better image
        clearer, brighter
        less ghosting
            ghosting is caused when some of the wrong image bleeds through to
                the other eye
            can also be caused by pixel persistence in lower quality LCD's

Anaglyph Stereo
    red cyan glasses
    render scene twice
        first pass - left - convert to grayscale then red
        second pass - right - convert to grayscale then cyan
        easiest to deploy

Other Stereo-limiting Factors
    right and left eye images do not match in color, size, vertical alignment
    distortion caused by the optical system
    resolution
    HMD's interocular settings
    computational model does not match viewing geometry
    some people can't see stereoscopic images (their brain does not recognize
        stereoscopic information


Thu Mar 17 12:32:37 EDT 2016
----------------------------

Asymmetric Frustrum
    the "correct" perspective through a window is an asymmetric frustrum
        left and right field of view are not the same relatively

Computing the Prejection Function (matrix)
    Assumptions
        a rectangular display
        a pinhole camera model (i.e. no distortion)
        known transform for the eye and display relative to the world origin
        known width and height of display
    determine the eye position in the display
        T[display, world] * T[world, eye] * P[eye]
    assign a near and far plane distance

Unity Projection Matrix
    camera.projectionmatrix
        unity docs provide sample asymmetric frustrum code
    provides 3 rendering displays simultaneously
        near plane: things closer than the physical display (3d "pop" effect)
        display: the window to the virtual world
        far plane: things past the window
    in unity, by default the near plane must be adjusted in code to
        differentiate it from the display

Stereoscopic HMD Setup
    2 displays coupled with a (generally) fixed transform
        not necessarily planar
    2 eyes, represented by cameras
    pop-out effect only if near plane is closer than the display

Lens Distortion
    lenses (especially cheap ones) can distort the image significantly from the
        baseline of a pinhole camera
    to compensate, you use better lenses or try to pre-distort the image to
        compensate
    using barrel distortion shrinks the images, leaving black bars around image
        to get around this, we must render a larger image
        this is part of why oculus is having resolution issues
    refraction depends on the frequency of light, called chromatic aberration
        typical glass lenses refract higher frequency blue light more than
            lower frequency red light
        higher distortion for higher frequency must be used to avoid red
            borders at the edges


Thu Mar 24 12:40:19 EDT 2016
----------------------------

look into: TVGG website, spacepoint fusion tracker

Definitions
    Presence
        the extent of feeling "there" in the virtual world (as opposed to the
            physical world)
    Co-presence
        the extent of feeling "with" another person in the virtual world
    Immersion
        ON SLIDES
    Place Illusion (PI)
        user perceives body as located in the virtual environment
        bounded by immersion
        your actions affet what you observe as you would expect
        your ability to function in the virtual world
        if it seems like you should be able to do something, you can do it
            for example: raising your hand and seeing it raise at the same time
            in the game
            if it seems like you should be able to do something, you can do it
                for example: raising your hand and seeing it raise at the same
                    time in the game
    Plausibility Illusion (PSI)
        believing that the game world is real
            for example: if someone swings at you and you duck in real life
        ascribing real traits to virtual things

Measuring Presence
    all measures of presence (PI or PSI) are indirect
    three general classes of presence tests:
        self-reported
            ex: surveys
            generally on some sort of scale of one to n
        physiological
            ex: sweating, pulse, blood pressure, pupil dilation, skin temp
        performance/behavior
            generally have to be built into application, tend to be
                app-specific
            ex: measures of reactions
            task performance
                finishing a puzzle faster
                solving a math problem correctly
            suspension of belief
                instinctual response
                    ducking at boxer's punch
                    snese of virtigo
                socially conditioned reactions
                    politeness
                    proxemics
                        study of distances
                            don't get closer than normal to other people or
                            dangerous things
                        proxemics can be affected by, for example, compression
                            issues in a HMD
    things that increase presence:
        low latency
        head tracking
        field of view
        rendering
            poly count
            shadows
            lighting
        multiple senses
            audio
            haptics
                includes "passive haptics" which are just phyisical props
        travel technique
            ex: real walking vs control stick
        avatar
            how the persons body is represented in the virtual world
    things that decrease presence:
        high latency
        disjoint senses
            expectation vs experience
        disembodied voice
        cables
        running into walls
        outside audio
        these are called 'breaks in presence' (BIPs)

Cued Gestalt
    we enter the virtual environment carrying the baggage of our experience,
        beliefs, fears, and expectation
    what we bring to the VE is as important as what we find there
    UVA (Pausch) star wars example
        poor VR w/ no backstory is much worse than poor VR with back story
        if shown the world of the environment before entering the environment,
            presence is increased
            we can attach previous exposure experience to our interpretation of
                the virtualized world

Pit Experiment
    most commonly replicated presence test experiment
    designed to test sense of presence
    begins in a staging room
        normal virtual room with a door
        told that there is a pit in the next room
        objective is to drop a ball and hit a target in the pit from both sides
    despite knowing there is no pit, people will creep around the edge and put
        out arms to avoid falling

Open Questions About Presence
    does it matter?
        does increased presence improve performance?
    can you have presence in a desktop VR?
    what about mixed reality and augmented reality?
    what seems to be true?
        we know virtual reality to be consequence-poor
            virtual environment does not accurately or completely represent the
                real world situation because of this
        illusion of reality can be increased through cued gestalt, better tech,
            and more realistic behavior of VEs
        a person's berceptions of real-world situations and behavior in the
            real world may be modified based on experiences in a virtual world
            ex: immersion therapy for phobias
        not only can we cause reactions, but we can change them in the future

More on VRPN
    must be on the same network
    must use the same name, ip address and port
    turn off all firewalls
    don't use PAWS-secure (maybe phone tethered network)
    make sure each side of the server can ping the other

